{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model/region_proposal_network.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionProposalNetwork(nn.Module):\n",
    "    \"\"\"Region Proposal Network introduced in Faster R-CNN.\n",
    "    This is Region Proposal Network introduced in Faster R-CNN [#]_.\n",
    "    This takes features extracted from images and propose\n",
    "    class agnostic bounding boxes around \"objects\".\n",
    "    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \\\n",
    "    Faster R-CNN: Towards Real-Time Object Detection with \\\n",
    "    Region Proposal Networks. NIPS 2015.\n",
    "    Args:\n",
    "        in_channels (int): The channel size of input.\n",
    "        mid_channels (int): The channel size of the intermediate tensor.\n",
    "        ratios (list of floats): This is ratios of width to height of\n",
    "            the anchors.\n",
    "        anchor_scales (list of numbers): This is areas of anchors.\n",
    "            Those areas will be the product of the square of an element in\n",
    "            :obj:`anchor_scales` and the original area of the reference\n",
    "            window.\n",
    "        feat_stride (int): Stride size after extracting features from an\n",
    "            image.\n",
    "        initialW (callable): Initial weight value. If :obj:`None` then this\n",
    "            function uses Gaussian distribution scaled by 0.1 to\n",
    "            initialize weight.\n",
    "            May also be a callable that takes an array and edits its values.\n",
    "        proposal_creator_params (dict): Key valued paramters for\n",
    "            :class:`model.utils.creator_tools.ProposalCreator`.\n",
    "    .. seealso::\n",
    "        :class:`~model.utils.creator_tools.ProposalCreator`\n",
    "    \"\"\"\n",
    "    1.计算回归坐标偏移量\n",
    "    2.计算anchors前景概率\n",
    "    3.调用ProposalCreator和使用NMS得出2000个近似目标框的坐标\n",
    "\n",
    "    def __init__(\n",
    "            self, in_channels=512, mid_channels=512, ratios=[0.5, 1, 2],\n",
    "            anchor_scales=[8, 16, 32], feat_stride=16,\n",
    "            proposal_creator_params=dict(),\n",
    "    ):\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        #调用generate_anchor_base（）函数，生成左上角9个anchor_base\n",
    "        self.anchor_base = generate_anchor_base(\n",
    "            anchor_scales=anchor_scales, ratios=ratios)\n",
    "        self.feat_stride = feat_stride\n",
    "        self.proposal_layer = ProposalCreator(self, **proposal_creator_params) #输出2000roi\n",
    "        n_anchor = self.anchor_base.shape[0] #9个\n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1)\n",
    "        self.score = nn.Conv2d(mid_channels, n_anchor * 2, 1, 1, 0)\n",
    "        self.loc = nn.Conv2d(mid_channels, n_anchor * 4, 1, 1, 0)\n",
    "        #作归一化处理\n",
    "        normal_init(self.conv1, 0, 0.01)\n",
    "        normal_init(self.score, 0, 0.01)\n",
    "        normal_init(self.loc, 0, 0.01)\n",
    "\n",
    "    def forward(self, x, img_size, scale=1.):\n",
    "        \"\"\"Forward Region Proposal Network.\n",
    "        Here are notations.\n",
    "        * :math:`N` is batch size.\n",
    "        * :math:`C` channel size of the input.\n",
    "        * :math:`H` and :math:`W` are height and witdh of the input feature.\n",
    "        * :math:`A` is number of anchors assigned to each pixel.\n",
    "        Args:\n",
    "            x (~torch.autograd.Variable): The Features extracted from images.\n",
    "                Its shape is :math:`(N, C, H, W)`.\n",
    "            img_size (tuple of ints): A tuple :obj:`height, width`,\n",
    "                which contains image size after scaling.\n",
    "            scale (float): The amount of scaling done to the input images after\n",
    "                reading them from files.\n",
    "        Returns:\n",
    "            (~torch.autograd.Variable, ~torch.autograd.Variable, array, array, array):\n",
    "            This is a tuple of five following values.\n",
    "            * **rpn_locs**: Predicted bounding box offsets and scales for \\\n",
    "                anchors. Its shape is :math:`(N, H W A, 4)`.\n",
    "            * **rpn_scores**:  Predicted foreground scores for \\\n",
    "                anchors. Its shape is :math:`(N, H W A, 2)`.\n",
    "            * **rois**: A bounding box array containing coordinates of \\\n",
    "                proposal boxes.  This is a concatenation of bounding box \\\n",
    "                arrays from multiple images in the batch. \\\n",
    "                Its shape is :math:`(R', 4)`. Given :math:`R_i` predicted \\\n",
    "                bounding boxes from the :math:`i` th image, \\\n",
    "                :math:`R' = \\\\sum _{i=1} ^ N R_i`.\n",
    "            * **roi_indices**: An array containing indices of images to \\\n",
    "                which RoIs correspond to. Its shape is :math:`(R',)`.\n",
    "            * **anchor**: Coordinates of enumerated shifted anchors. \\\n",
    "                Its shape is :math:`(H W A, 4)`.\n",
    "        \"\"\"\n",
    "        n, _, hh, ww = x.shape #(batch_size，512,H/16,W/16），其中H，W分别为原图的高和宽\n",
    "        anchor = _enumerate_shifted_anchor(\n",
    "            np.array(self.anchor_base),\n",
    "            self.feat_stride, hh, ww) #在9个base_anchor基础上生成hh*ww*9个anchor，对应到原图坐标\n",
    "\n",
    "        n_anchor = anchor.shape[0] // (hh * ww)  #hh*ww*9/hh*ww=9\n",
    "        h = F.relu(self.conv1(x)) #512个3x3卷积(512, H/16,W/16)\n",
    "\n",
    "        rpn_locs = self.loc(h) #n_anchor（9）*4个1x1卷积，回归坐标偏移量。（9*4，hh,ww）\n",
    "\n",
    "        #转换为（n，hh，ww，9*4）后变为（n，hh*ww*9，4）\n",
    "        rpn_locs = rpn_locs.permute(0, 2, 3, 1).contiguous().view(n, -1, 4)\n",
    "        rpn_scores = self.score(h) #n_anchor（9）*2个1x1卷积，回归类别。（9*2，hh,ww）\n",
    "        rpn_scores = rpn_scores.permute(0, 2, 3, 1).contiguous() #转换为（n，hh，ww，9*2）\n",
    "        rpn_softmax_scores = F.softmax(rpn_scores.view(n, hh, ww, n_anchor, 2), dim=4) #计算softmax\n",
    "        rpn_fg_scores = rpn_softmax_scores[:, :, :, :, 1].contiguous() #得到前景的分类概率\n",
    "        rpn_fg_scores = rpn_fg_scores.view(n, -1) #得到所有anchor的前景分类概率\n",
    "        rpn_scores = rpn_scores.view(n, -1, 2) #得到每一张feature map上所有anchor的网络输出值\n",
    "\n",
    "        rois = list()\n",
    "        roi_indices = list()\n",
    "        for i in range(n): #n为batch_size数\n",
    "            \n",
    "        # 调用ProposalCreator函数， rpn_locs维度（hh*ww*9，4），rpn_fg_scores维度为（hh*ww*9），\n",
    "        #anchor的维度为（hh*ww*9，4）， img_size的维度为（3，H，W），H和W是经过数据预处理后的。\n",
    "        #计算（H/16）x(W/16)x9(大概20000)个anchor属于前景的概率，取前12000个并经过NMS得到2000个近似目标框的坐标。\n",
    "  \n",
    "            roi = self.proposal_layer(\n",
    "                rpn_locs[i].cpu().data.numpy(), #位置信息\n",
    "                rpn_fg_scores[i].cpu().data.numpy(), #前景概率\n",
    "                anchor, img_size,\n",
    "                scale=scale)\n",
    "            batch_index = i * np.ones((len(roi),), dtype=np.int32)\n",
    "            rois.append(roi)\n",
    "            roi_indices.append(batch_index)\n",
    "\n",
    "        rois = np.concatenate(rois, axis=0) #rois为所有batch_size的roi\n",
    "        roi_indices = np.concatenate(roi_indices, axis=0) #按行拼接\n",
    "        #rpn_locs的维度（hh*ww*9，4），rpn_scores维度为（hh*ww*9，2）， \n",
    "        #rois的维度为（2000,4），roi_indices用不到，anchor的维度为（hh*ww*9，4）\n",
    "        return rpn_locs, rpn_scores, rois, roi_indices, anchor \n",
    "\n",
    "\n",
    "def _enumerate_shifted_anchor(anchor_base, feat_stride, height, width):\n",
    "    #在整个特征图生成所有的anchor，对应回原图大小\n",
    "\n",
    "    import numpy as xp\n",
    "    shift_y = xp.arange(0, height * feat_stride, feat_stride)#纵向偏移量（0，16，32，...）\n",
    "    shift_x = xp.arange(0, width * feat_stride, feat_stride)# 横向偏移量（0，16，32，...）\n",
    "     #shift_x = [[0，16，32，..],[0，16，32，..],[0，16，32，..]...],\n",
    "     #shift_y = [[0，0，0，..],[16，16，16，..],[32，32，32，..]...],\n",
    "    #就是形成了一个纵横向偏移量的矩阵，也就是特征图的每一点都能够通过这个矩阵找到映射在原图中的具体位置！\n",
    "    shift_x, shift_y = xp.meshgrid(shift_x, shift_y)\n",
    "    #产生偏移坐标对，一个朝x方向，一个朝y方向偏移。\n",
    "    shift = xp.stack((shift_y.ravel(), shift_x.ravel(),\n",
    "                      shift_y.ravel(), shift_x.ravel()), axis=1)\n",
    "\n",
    "    A = anchor_base.shape[0] #A=9\n",
    "    K = shift.shape[0] #读取特征图中元素的总个数\n",
    "    anchor = anchor_base.reshape((1, A, 4)) + \\\n",
    "             shift.reshape((1, K, 4)).transpose((1, 0, 2))\n",
    "    anchor = anchor.reshape((K * A, 4)).astype(np.float32)\n",
    "    return anchor\n",
    "\n",
    "\n",
    "def _enumerate_shifted_anchor_torch(anchor_base, feat_stride, height, width):\n",
    "    # Enumerate all shifted anchors:\n",
    "    #\n",
    "    # add A anchors (1, A, 4) to\n",
    "    # cell K shifts (K, 1, 4) to get\n",
    "    # shift anchors (K, A, 4)\n",
    "    # reshape to (K*A, 4) shifted anchors\n",
    "    # return (K*A, 4)\n",
    "\n",
    "    # !TODO: add support for torch.CudaTensor\n",
    "    # xp = cuda.get_array_module(anchor_base)\n",
    "    import torch as t\n",
    "    shift_y = t.arange(0, height * feat_stride, feat_stride)\n",
    "    shift_x = t.arange(0, width * feat_stride, feat_stride)\n",
    "    shift_x, shift_y = xp.meshgrid(shift_x, shift_y)\n",
    "    shift = xp.stack((shift_y.ravel(), shift_x.ravel(),\n",
    "                      shift_y.ravel(), shift_x.ravel()), axis=1)\n",
    "\n",
    "    A = anchor_base.shape[0]\n",
    "    K = shift.shape[0]\n",
    "    anchor = anchor_base.reshape((1, A, 4)) + \\\n",
    "             shift.reshape((1, K, 4)).transpose((1, 0, 2))\n",
    "    anchor = anchor.reshape((K * A, 4)).astype(np.float32)\n",
    "    return anchor\n",
    "\n",
    "\n",
    "def normal_init(m, mean, stddev, truncated=False):\n",
    "    \"\"\"\n",
    "    weight initalizer: truncated normal and random normal.\n",
    "    \"\"\"\n",
    "    # x is a parameter\n",
    "    if truncated:\n",
    "        m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean)  # not a perfect approximation\n",
    "    else:\n",
    "        m.weight.data.normal_(mean, stddev)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decom_vgg16():\n",
    "    # the 30th layer of features is relu of conv5_3\n",
    "    if opt.caffe_pretrain: #是否使用caffe预训练\n",
    "        model = vgg16(pretrained=False)\n",
    "        if not opt.load_path:\n",
    "            model.load_state_dict(t.load(opt.caffe_pretrain_path))\n",
    "    else:\n",
    "        model = vgg16(not opt.load_path)\n",
    "\n",
    "    features = list(model.features)[:30] #加载预训练模型vgg16的conv5_3之前的部分\n",
    "    classifier = model.classifier\n",
    "\n",
    "    classifier = list(classifier)\n",
    "    del classifier[6] #删除最后分1000类\n",
    "    if not opt.use_drop: #删除两个dropout\n",
    "        del classifier[5]\n",
    "        del classifier[2]\n",
    "    classifier = nn.Sequential(*classifier)\n",
    "\n",
    "    for layer in features[:10]: #冻结vgg16前2个stage,不进行反向传播\n",
    "        for p in layer.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    return nn.Sequential(*features), classifier #拆分为特征提取网络和分类网络\n",
    "\n",
    "\n",
    "class FasterRCNNVGG16(FasterRCNN):\n",
    "    \"\"\"Faster R-CNN based on VGG-16.\n",
    "    For descriptions on the interface of this model, please refer to\n",
    "    :class:`model.faster_rcnn.FasterRCNN`.\n",
    "    Args:\n",
    "        n_fg_class (int): The number of classes excluding the background.\n",
    "        ratios (list of floats): This is ratios of width to height of\n",
    "            the anchors.\n",
    "        anchor_scales (list of numbers): This is areas of anchors.\n",
    "            Those areas will be the product of the square of an element in\n",
    "            :obj:`anchor_scales` and the original area of the reference\n",
    "            window.\n",
    "    \"\"\"\n",
    "    #分别对特征VGG16的特征提取部分、分类部分、RPN网络、VGG16RoIHead网络进行了实例化\n",
    "    feat_stride = 16  # downsample 16x for output of conv5 in vgg16\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_fg_class=20,\n",
    "                 ratios=[0.5, 1, 2],\n",
    "                 anchor_scales=[8, 16, 32]\n",
    "                 ): #总类别数为20类，三种尺度三种比例的anchor\n",
    "                 \n",
    "        extractor, classifier = decom_vgg16() #conv5_3及之前的部分，分类器\n",
    "\n",
    "        rpn = RegionProposalNetwork(\n",
    "            512, 512,\n",
    "            ratios=ratios,\n",
    "            anchor_scales=anchor_scales,\n",
    "            feat_stride=self.feat_stride,\n",
    "        ) #返回rpn_locs, rpn_scores, rois, roi_indices, anchor\n",
    "\n",
    "        head = VGG16RoIHead(\n",
    "            n_class=n_fg_class + 1,\n",
    "            roi_size=7,\n",
    "            spatial_scale=(1. / self.feat_stride),\n",
    "            classifier=classifier\n",
    "        ) #下面会分析VGG16RoIHead（），n_class = 21（加上背景）\n",
    "\n",
    "        super(FasterRCNNVGG16, self).__init__(\n",
    "            extractor,\n",
    "            rpn,\n",
    "            head,\n",
    "        ) #相当于给faster_rcnn传入参数extractor, rpn, head\n",
    "\n",
    "\n",
    "class VGG16RoIHead(nn.Module):\n",
    "    \"\"\"Faster R-CNN Head for VGG-16 based implementation.\n",
    "    This class is used as a head for Faster R-CNN.\n",
    "    This outputs class-wise localizations and classification based on feature\n",
    "    maps in the given RoIs.\n",
    "    \n",
    "    Args:\n",
    "        n_class (int): The number of classes possibly including the background.\n",
    "        roi_size (int): Height and width of the feature maps after RoI-pooling.\n",
    "        spatial_scale (float): Scale of the roi is resized.\n",
    "        classifier (nn.Module): Two layer Linear ported from vgg16\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_class, roi_size, spatial_scale,\n",
    "                 classifier):\n",
    "        # n_class includes the background\n",
    "        super(VGG16RoIHead, self).__init__()\n",
    "\n",
    "        self.classifier = classifier #vgg16中的classifier\n",
    "        self.cls_loc = nn.Linear(4096, n_class * 4)\n",
    "        self.score = nn.Linear(4096, n_class)\n",
    "\n",
    "        normal_init(self.cls_loc, 0, 0.001)\n",
    "        normal_init(self.score, 0, 0.01) #全连接层权重初始化\n",
    "\n",
    "        self.n_class = n_class #加上背景21类\n",
    "        self.roi_size = roi_size #7\n",
    "        self.spatial_scale = spatial_scale # 1/16\n",
    "        #将大小不同的roi变成大小一致，得到pooling后的特征，大小为[300, 512, 7, 7]。.利用Cupy实现在线编译的\n",
    "        #使用的是from torchvision.ops import RoIPool\n",
    "        self.roi = RoIPool( (self.roi_size, self.roi_size),self.spatial_scale) \n",
    "    def forward(self, x, rois, roi_indices):\n",
    "        \"\"\"Forward the chain.\n",
    "        We assume that there are :math:`N` batches.\n",
    "        Args:\n",
    "            x (Variable): 4D image variable.\n",
    "            rois (Tensor): A bounding box array containing coordinates of\n",
    "                proposal boxes.  This is a concatenation of bounding box\n",
    "                arrays from multiple images in the batch.\n",
    "                Its shape is :math:`(R', 4)`. Given :math:`R_i` proposed\n",
    "                RoIs from the :math:`i` th image,\n",
    "                :math:`R' = \\\\sum _{i=1} ^ N R_i`.\n",
    "            roi_indices (Tensor): An array containing indices of images to\n",
    "                which bounding boxes correspond to. Its shape is :math:`(R',)`.\n",
    "        \"\"\"\n",
    "        # in case roi_indices is  ndarray\n",
    "        roi_indices = at.totensor(roi_indices).float() #ndarray->tensor\n",
    "        rois = at.totensor(rois).float()\n",
    "        indices_and_rois = t.cat([roi_indices[:, None], rois], dim=1)\n",
    "        # NOTE: important: yx->xy\n",
    "        xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]\n",
    "        indices_and_rois =  xy_indices_and_rois.contiguous() #把tensor变成在内存中连续分布的形式\n",
    "\n",
    "        pool = self.roi(x, indices_and_rois) #ROIPOOLING\n",
    "        pool = pool.view(pool.size(0), -1)\n",
    "        fc7 = self.classifier(pool) #decom_vgg16（）得到的calssifier,得到4096\n",
    "        roi_cls_locs = self.cls_loc(fc7) #（4096->84） 84=21*4\n",
    "        roi_scores = self.score(fc7) #（4096->21）\n",
    "        return roi_cls_locs, roi_scores\n",
    "\n",
    "\n",
    "def normal_init(m, mean, stddev, truncated=False):\n",
    "    \"\"\"\n",
    "    weight initalizer: truncated normal and random normal.\n",
    "    \"\"\"\n",
    "    # x is a parameter\n",
    "    if truncated:\n",
    "        m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean)  # not a perfect approximation\n",
    "    else:\n",
    "        m.weight.data.normal_(mean, stddev)\n",
    "        m.bias.data.zero_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
